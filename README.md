<h1 align="center">ğŸ¤– Production-Ready RAG AI Agent</h1>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.11+-blue?style=for-the-badge&logo=python&logoColor=white" />
  <img src="https://img.shields.io/badge/RAG-Retrieval--Augmented%20Generation-purple?style=for-the-badge" />
  <img src="https://img.shields.io/badge/Orchestration-Inngest-FF6B35?style=for-the-badge" />
  <img src="https://img.shields.io/badge/UI-Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white" />
  <img src="https://img.shields.io/badge/Vector%20DB-Semantic%20Search-green?style=for-the-badge" />
  <img src="https://img.shields.io/badge/Status-Production%20Ready-brightgreen?style=for-the-badge" />
</p>

<p align="center">
  A <strong>production-grade Retrieval-Augmented Generation (RAG) AI Agent</strong> built with Python â€” featuring real-world engineering concerns like observability, retries, rate limiting, orchestration, and a full Streamlit UI. This is not a demo script â€” it's an end-to-end deployable AI service.
</p>

---

## ğŸ¬ Inspiration & Credit

> This project was built following and extending the concepts from the excellent YouTube tutorial:
>
> ğŸ“º **[Production-Ready RAG AI Agent â€” YouTube Tutorial](https://www.youtube.com/watch?v=AUQJ9eeP-Ls)**
>
> Huge credit to the creator for the clear and practical walkthrough. This repo applies those concepts, structures the code for real deployment, and adds additional production enhancements.

---

## ğŸ§  Why This Project Stands Out

Most RAG projects are Jupyter notebooks or simple scripts. This repo is architected **like a real production service** â€” the way an engineering team at a company would actually build and ship it.

| Feature | Typical RAG Demo | This Project |
|---|---|---|
| Observability & Logging | âŒ | âœ… |
| Retry Logic | âŒ | âœ… |
| Rate Limiting | âŒ | âœ… |
| Workflow Orchestration | âŒ | âœ… Inngest |
| Vector DB Integration | Basic | âœ… Modular |
| Interactive UI | âŒ | âœ… Streamlit |
| Modular Architecture | âŒ | âœ… |
| Deployment Ready | âŒ | âœ… |

---

## ğŸ—ï¸ Architecture Overview
```
User Query
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Streamlit Frontend          â”‚
â”‚         (streamlit_app.py)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Inngest Orchestrator         â”‚
â”‚  (Retries / Rate Limits / Logging)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vector DB  â”‚   â”‚   LLM / OpenAI   â”‚
â”‚ Retrieval  â”‚   â”‚  Response Gen    â”‚
â”‚(vector_db) â”‚   â”‚   (main.py)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–²
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Loader  â”‚
â”‚(data_loader) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ¨ Key Features

- ğŸ” **Retry Policies** â€” Automatic exponential backoff on flaky LLM/API calls
- ğŸš¦ **Rate Limiting** â€” Guards against API overuse and runaway costs
- ğŸ‘ï¸ **Observability** â€” Structured logging and debug-friendly outputs throughout
- âš™ï¸ **Inngest Orchestration** â€” Serverless-style durable workflow management
- ğŸ—ƒï¸ **Vector Database** â€” Fast semantic search over ingested documents
- ğŸ–¥ï¸ **Streamlit UI** â€” Clean, interactive frontend for querying the RAG agent
- ğŸ§© **Modular Design** â€” Every component (loader, vector DB, LLM, UI) is independently swappable
- ğŸ **Modern Python** â€” Uses `pyproject.toml`, `uv` lock file, and pinned Python version

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|---|---|
| Language | Python 3.11+ |
| Workflow Orchestration | [Inngest](https://www.inngest.com/) |
| LLM Backend | OpenAI API |
| Vector Store | Configurable (ChromaDB / Pinecone / etc.) |
| Frontend | Streamlit |
| Dependency Management | `uv` + `pyproject.toml` |
| Logging | Python standard logging |
| Deployment | Docker / Cloud-ready |

---

## ğŸ“ Project Structure
```
Production-Ready_RAG_AI_Agent/
â”‚
â”œâ”€â”€ main.py                 # Core RAG pipeline & Inngest functions
â”œâ”€â”€ streamlit_app.py        # Interactive Streamlit frontend
â”œâ”€â”€ data_loader.py          # Document ingestion & preprocessing
â”œâ”€â”€ vector_db.py            # Vector store setup & semantic search
â”œâ”€â”€ custom_types.py         # Pydantic models & shared data types
â”œâ”€â”€ pyproject.toml          # Project dependencies & config
â”œâ”€â”€ uv.lock                 # Locked dependency versions
â”œâ”€â”€ .python-version         # Pinned Python version
â”œâ”€â”€ .env.sample             # Environment variable template
â”œâ”€â”€ inngest_function.jpg    # Inngest dashboard screenshot
â”œâ”€â”€ inngest_working.jpg     # Inngest workflow diagram
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### 1. Clone the Repository
```bash
git clone https://github.com/krutipandy/Production-Ready_RAG_AI_Agent.git
cd Production-Ready_RAG_AI_Agent
```

### 2. Set Up Environment
```bash
# Copy the environment template
cp .env.sample .env
```

Edit `.env` and fill in your credentials:
```env
OPENAI_API_KEY=your_openai_key_here
INNGEST_EVENT_KEY=your_inngest_event_key
INNGEST_SIGNING_KEY=your_inngest_signing_key
VECTOR_DB_URL=your_vector_db_connection
```

### 3. Install Dependencies

**Using `uv` (recommended):**
```bash
pip install uv
uv sync
```

**Using pip:**
```bash
pip install -r requirements.txt
```

### 4. Ingest Your Data
```bash
python data_loader.py
```

### 5. Run the Backend
```bash
python main.py
```

### 6. Launch the Frontend
```bash
streamlit run streamlit_app.py
```

Open your browser at `http://localhost:8501` and start querying your AI agent!

---

## âš™ï¸ Inngest Orchestration

This project uses [Inngest](https://www.inngest.com/) to handle the RAG pipeline as a **durable, observable workflow** â€” not a fragile chain of function calls.

**Why Inngest?**
- Each step is logged and visible in a live dashboard
- If an LLM call fails, Inngest automatically retries with backoff
- Rate limiting is enforced at the orchestration layer
- Steps can be paused, replayed, and debugged individually

### Inngest Dashboard
![Inngest Function](inngest_function.jpg)

### Inngest Workflow in Action
![Inngest Working](inngest_working.jpg)

---

## ğŸ”„ How RAG Works in This Project
```
1. INGESTION
   Documents â†’ data_loader.py â†’ Chunked & Embedded â†’ Vector DB

2. RETRIEVAL
   User Query â†’ Embedded â†’ Semantic Search â†’ Top-K Relevant Chunks

3. GENERATION
   Relevant Chunks + User Query â†’ LLM Prompt â†’ AI Response

4. ORCHESTRATION (via Inngest)
   Each step above is a durable function with retries, logging & rate limits
```

---

## ğŸŒ Deployment Guide

### Docker (Recommended)
```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY . .
RUN pip install uv && uv sync
EXPOSE 8501
CMD ["streamlit", "run", "streamlit_app.py", "--server.port=8501"]
```
```bash
docker build -t rag-ai-agent .
docker run -p 8501:8501 --env-file .env rag-ai-agent
```

### Cloud Deployment Options
- **AWS** â€” ECS / Lambda + API Gateway
- **GCP** â€” Cloud Run
- **Azure** â€” Container Apps
- **Railway / Render** â€” One-click deploy from GitHub

---

## ğŸ’¡ Use Cases

- ğŸ“„ **Document Q&A** â€” Chat with PDFs, reports, and knowledge bases
- ğŸ¢ **Enterprise Search** â€” Internal knowledge retrieval with LLM answers
- ğŸ“ **Education Tools** â€” Course material Q&A assistant
- ğŸ› ï¸ **Developer Tools** â€” Code documentation assistant
- ğŸ©º **Healthcare / Legal** â€” Retrieval over specialized domain documents

---

## ğŸ”­ Future Enhancements

- [ ] Add **multi-document support** with metadata filtering
- [ ] Integrate **LangGraph** for advanced agent reasoning loops
- [ ] Add **evaluation pipeline** (RAGAS / TruLens) for answer quality scoring
- [ ] Implement **streaming responses** in Streamlit UI
- [ ] Add **authentication** layer for multi-user deployments
- [ ] Build **CI/CD pipeline** with GitHub Actions

---

## ğŸ‘©â€ğŸ’» Author

**Kruti Pandya**  
AI/ML Engineer | LLM & RAG Systems | Production AI Applications

[![GitHub](https://img.shields.io/badge/GitHub-krutipandy-black?style=flat&logo=github)](https://github.com/krutipandy)

---

## ğŸ™ Acknowledgements

- ğŸ“º **Tutorial Credit:** [Production-Ready RAG AI Agent â€” YouTube](https://www.youtube.com/watch?v=AUQJ9eeP-Ls) â€” for the foundational concepts and walkthrough that inspired this project.
- âš™ï¸ [Inngest](https://www.inngest.com/) â€” for the powerful workflow orchestration platform
- ğŸ¤– [OpenAI](https://openai.com/) â€” for the LLM backbone
- ğŸ–¥ï¸ [Streamlit](https://streamlit.io/) â€” for the rapid UI framework

---

## ğŸ“„ License

This project is open-source and available under the [MIT License](LICENSE).

---

<p align="center">â­ If this project helped you understand production RAG systems, please give it a star!</p>
